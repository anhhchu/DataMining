{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3R-Mrl4c-Ro9"
   },
   "source": [
    "## Association Rule Mining \n",
    "Association rule mining is a technique to identify underlying relations between different items. Take an example of a Super Market where customers can buy variety of items. Usually, there is a pattern in what the customers buy. For instance, mothers with babies buy baby products such as milk and diapers.\n",
    "\n",
    "For instance, if item A and B are bought together more frequently then several steps can be taken to increase the profit. For example:\n",
    "\n",
    "1. A and B can be placed together so that when a customer buys one of the product he doesn't have to go far away to buy the other product.\n",
    "1. People who buy one of the products can be targeted through an advertisement campaign to buy the other.\n",
    "1. Collective discounts can be offered on these products if the customer buys both of them.\n",
    "1. Both A and B can be packaged together.\n",
    "The process of identifying an associations between products is called association rule mining.\n",
    "\n",
    "The most prominent practical application of the algorithm is to recommend products based on the products already present in the user’s cart. Walmart especially has made great use of the algorithm in suggesting products to it’s users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJB33NT3Zveo"
   },
   "source": [
    "### Apriori Algorithm for Association Rule Mining\n",
    "\n",
    "Different statistical algorithms have been developed to implement association rule mining, and Apriori is one such algorithm. \n",
    "\n",
    "Apriori algorithm considers 3 important factors which are, support, confidence and lift. Each of these factors is explained as follows:\n",
    "\n",
    "1. Support:\n",
    "The support of item I is defined as the ratio between the number of transactions containing the item I by the total number of transactions expressed as :\n",
    "$Support(I) = \\frac{\\text{number of  transactions containing I}}{\\text{Total number of transactions}}$\n",
    "1. Confidence:\n",
    "Confidence refers to the likelihood that an item B is also bought if item A is bought. It can be calculated by finding the number of transactions where A and B are bought together, divided by total number of transactions where A is bought.\n",
    "$Confidence(A→B) = \\frac{\\text{number of transactions containing both (A and B)}}{\\text{number of transactions containing A}}$\n",
    "1. Lift:\n",
    "Lift(A -> B) refers to the increase in the ratio of sale of B when A is sold. \n",
    "$Lift(A→B) = \\frac{\\text{Confidence (A→B)}}{\\text{Support (B)}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0dSWoGxRaqp"
   },
   "source": [
    "### 1. Installation\n",
    "[Mlxtend](http://rasbt.github.io/mlxtend/) (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T21:19:02.930071Z",
     "iopub.status.busy": "2022-01-25T21:19:02.929589Z",
     "iopub.status.idle": "2022-01-25T21:21:40.891174Z",
     "shell.execute_reply": "2022-01-25T21:21:40.890459Z",
     "shell.execute_reply.started": "2022-01-25T21:19:02.929954Z"
    },
    "id": "Sf9VJo529meK",
    "outputId": "83072092-4678-4409-cbf2-2949ad635ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (0.19.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from mlxtend) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from mlxtend) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from mlxtend) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from mlxtend) (52.0.0.post20210125)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from mlxtend) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from mlxtend) (1.20.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from mlxtend) (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
      "Requirement already satisfied: six in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.24.2->mlxtend) (2021.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.20.3->mlxtend) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mlxtend --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T21:39:24.366992Z",
     "iopub.status.busy": "2022-01-25T21:39:24.366426Z",
     "iopub.status.idle": "2022-01-25T21:39:24.405728Z",
     "shell.execute_reply": "2022-01-25T21:39:24.404933Z",
     "shell.execute_reply.started": "2022-01-25T21:39:24.366855Z"
    },
    "id": "77KO_1MXA4SG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, fpmax, fpgrowth, association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFmFbbFRRdH4"
   },
   "source": [
    "### Importing the Dataset\n",
    "Now let's import the dataset and see what we're working with.\n",
    "\n",
    "* Dataset : [Groceries data](http://archive.ics.uci.edu/ml/machine-learning-databases/00352/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T21:35:06.025032Z",
     "iopub.status.busy": "2022-01-25T21:35:06.024544Z",
     "iopub.status.idle": "2022-01-25T21:37:35.421896Z",
     "shell.execute_reply": "2022-01-25T21:37:35.420844Z",
     "shell.execute_reply.started": "2022-01-25T21:35:06.024992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (3.0.7)\n",
      "Requirement already satisfied: et-xmlfile in /Users/hoanganhchu/opt/anaconda3/lib/python3.8/site-packages (from openpyxl) (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T21:39:27.531787Z",
     "iopub.status.busy": "2022-01-25T21:39:27.531466Z",
     "iopub.status.idle": "2022-01-25T21:39:27.685561Z",
     "shell.execute_reply": "2022-01-25T21:39:27.684161Z",
     "shell.execute_reply.started": "2022-01-25T21:39:27.531752Z"
    },
    "id": "Qxk_yzqlG4uC",
    "outputId": "e5191d79-c8f0-4404-9781-0ed7af67f1ff"
   },
   "outputs": [],
   "source": [
    "# Loading the Data\n",
    "data = pd.read_excel('Online Retail.xlsx')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FGnPyaiQTYDX",
    "outputId": "131618ac-821a-4be2-eb82-769fcc530f28"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b32bf4e5bb67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Exploring the columns of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Exploring the columns of the data\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBJWXdtxTng7",
    "outputId": "118d9649-f64c-4e14-b701-76a5dd2f8062"
   },
   "outputs": [],
   "source": [
    "# Exploring the different regions of transactions\n",
    "data.Country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8ZbQyadgkwE",
    "outputId": "d27d843a-f39b-4b20-8679-285e4e2b277c"
   },
   "outputs": [],
   "source": [
    "data.groupby('Country').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RoR1ONRQ5Rh"
   },
   "source": [
    "### Data Proprocessing\n",
    "\n",
    "1. We will drop the rows without any invoice number\n",
    "1. We will drop all transactions which were done on credit\n",
    "1. Splitting the data according to the region of transaction \n",
    "1. Dropping all transactions which the frequency is less than 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnjBKHn-HPOF"
   },
   "outputs": [],
   "source": [
    "# Stripping extra spaces in the description\n",
    "data['Description'] = data['Description'].str.strip()\n",
    " \n",
    "# Dropping the rows without any invoice number\n",
    "data.dropna(axis = 0, subset =['InvoiceNo'], inplace = True)\n",
    "data['InvoiceNo'] = data['InvoiceNo'].astype('str')\n",
    " \n",
    "# Dropping all transactions which were done on credit\n",
    "data = data[~data['InvoiceNo'].str.contains('C')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xLu7UwWT28E",
    "outputId": "fed0307f-8ebf-43b0-def8-422131e8057a"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6XvfrNtQx03",
    "outputId": "4f34443c-b1ca-47bd-e1bd-be9a8f630454"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNb9Pu77SkLN"
   },
   "outputs": [],
   "source": [
    "min_freq = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCJc7UbNRlv_",
    "outputId": "ba2e3252-52ec-475f-de20-558aad8ef22d"
   },
   "outputs": [],
   "source": [
    "# Dropping all transactions which the frequency is less than 4\n",
    "count_item = data.groupby('Description').count().sort_values('InvoiceNo', ascending=False)\n",
    "count_item = count_item[count_item['InvoiceNo']>min_freq]\n",
    "print(count_item)\n",
    "\n",
    "data = data[data['Description'].isin(list(count_item.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlS3cpCLUlrA"
   },
   "outputs": [],
   "source": [
    "# Transactions done in USA\n",
    "basket_USA = (data[data['Country'] ==\"USA\"]\n",
    "          .groupby(['InvoiceNo', 'Description'])['Quantity']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('InvoiceNo'))\n",
    " \n",
    "# Transactions done in Canada\n",
    "basket_CAN = (data[data['Country'] ==\"Canada\"]\n",
    "          .groupby(['InvoiceNo', 'Description'])['Quantity']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('InvoiceNo'))\n",
    "\n",
    "# Transactions done in France\n",
    "basket_France = (data[data['Country'] ==\"France\"]\n",
    "          .groupby(['InvoiceNo', 'Description'])['Quantity']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('InvoiceNo'))\n",
    "\n",
    "# Transactions done in United Kingdom\n",
    "basket_UK = (data[data['Country'] ==\"United Kingdom\"]\n",
    "          .groupby(['InvoiceNo', 'Description'])['Quantity']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('InvoiceNo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5P-O50GVe9i",
    "outputId": "6fe21ce9-7c85-4833-b349-1c5bb3489917"
   },
   "outputs": [],
   "source": [
    "basket_UK.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlDe90CYVPib"
   },
   "outputs": [],
   "source": [
    "# Defining the hot encoding function to make the data suitable for the concerned libraries\n",
    "def hot_encode(x):\n",
    "    if(x<= 0):\n",
    "        return 0\n",
    "    if(x>= 1):\n",
    "        return 1\n",
    " \n",
    "# Encoding the datasets\n",
    "basket_encoded = basket_USA.applymap(hot_encode)\n",
    "basket_USA = basket_encoded\n",
    " \n",
    "basket_encoded = basket_CAN.applymap(hot_encode)\n",
    "basket_CAN = basket_encoded\n",
    " \n",
    "basket_encoded = basket_France.applymap(hot_encode)\n",
    "basket_France = basket_encoded\n",
    "\n",
    "basket_encoded = basket_UK.applymap(hot_encode)\n",
    "basket_UK = basket_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SU1Fm0dlVPMS"
   },
   "source": [
    "### Apply Apriori Algorithm\n",
    "For large sets of data, there can be hundreds of items in hundreds of thousands transactions. The Apriori algorithm tries to extract rules for each possible combination of items.\n",
    "\n",
    "This process can be extremely slow due to the number of combinations. To speed up the process, we need to perform the following steps:\n",
    "\n",
    "1. Set a minimum value for support and confidence. This means that we are only interested in finding rules for the items that have certain default existence (e.g. support) and have a minimum value for co-occurrence with other items (e.g. confidence).\n",
    "1. Extract all the subsets having higher value of support than minimum threshold.\n",
    "1. Select all the rules from the subsets with confidence value higher than minimum threshold.\n",
    "1. Order the rules by descending order of Lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXoV0MrjsFKc"
   },
   "outputs": [],
   "source": [
    "min_sup = 0.05\n",
    "min_conf = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7842sXeXx0M",
    "outputId": "a0ebd0cd-2218-4172-cb0f-63dc50fbf476"
   },
   "outputs": [],
   "source": [
    "# Now, let us return the items and itemsets with at least 5% support:\n",
    "# Building the aprior model\n",
    "frq_items = apriori(basket_France, min_support = min_sup, use_colnames = True)\n",
    "print(frq_items.head())\n",
    "print(frq_items.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egrN93JKlfNx",
    "outputId": "656d9d4e-7acf-45f5-8bff-bf27077e22b3"
   },
   "outputs": [],
   "source": [
    "# Building the FP growth model\n",
    "frq_items1 = fpgrowth(basket_France, min_support=min_sup, use_colnames=True)\n",
    "print(frq_items1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAB4dj-QJD2p",
    "outputId": "b8d537e2-978a-490d-d554-35e33ee57bf6"
   },
   "outputs": [],
   "source": [
    "frq_items1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNzN8gzIlLuN"
   },
   "source": [
    "1. Selecting and Filtering Results\n",
    "\n",
    "The advantage of working with pandas DataFrames is that we can use its convenient features to filter the results. \n",
    "For instance, let's assume we are only interested in itemsets of length 2 that have a support of at least 80 percent. First, we create the frequent itemsets via apriori and add a new column that stores the length of each itemset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdLFcTVgk3Aq",
    "outputId": "f5e9341f-66e4-4e94-eb45-770b74c527fe"
   },
   "outputs": [],
   "source": [
    "frq_items['length'] = frq_items['itemsets'].apply(lambda x: len(x))\n",
    "print(frq_items.head())\n",
    "frq_items1['length'] = frq_items1['itemsets'].apply(lambda x: len(x))\n",
    "print(frq_items1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmXKeMsurdZC",
    "outputId": "9a920e34-7ae8-45a6-b994-e40612d87901"
   },
   "outputs": [],
   "source": [
    "print(frq_items[ (frq_items['length'] == 2) & (frq_items['support'] >= min_sup) ])\n",
    "print(frq_items1[ (frq_items1['length'] == 2) & (frq_items1['support'] >= min_sup) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czSTnxpa092D"
   },
   "source": [
    "The generate_rules takes dataframes of frequent itemsets as produced by the `apriori`, `fpgrowth`, or `fpmax` functions in mlxtend.association. To demonstrate the usage of the generate_rules method, we first create a pandas DataFrame of frequent itemsets as generated by the `apriori` function:\n",
    "\n",
    "a) apriori model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9tW7dX5mMJ2",
    "outputId": "7094642b-56dd-43ee-9b9d-e81c1140bfab"
   },
   "outputs": [],
   "source": [
    "# Collecting the inferred rules in a dataframe\n",
    "rules = association_rules(frq_items, metric =\"confidence\", min_threshold = min_conf)\n",
    "rules = rules.sort_values(['confidence', 'lift'], ascending =[False, False])\n",
    "print(rules.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIuhlz7u1W-N"
   },
   "source": [
    "b) FP growth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zocbvpea0VxF",
    "outputId": "6d42a8f8-772c-45c7-8a41-d34397a4cefc"
   },
   "outputs": [],
   "source": [
    "rules1 = association_rules(frq_items1, metric =\"confidence\", min_threshold = min_conf)\n",
    "rules1 = rules1.sort_values(['confidence', 'lift'], ascending =[False, False])\n",
    "print(rules1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2lIkN_nRzNU"
   },
   "source": [
    "### Viewing the Results\n",
    "Pandas DataFrames make it easy to filter the results further. Let's say we are ony interested in rules that satisfy the following criteria:\n",
    "\n",
    "at least 2 antecedents\n",
    "a confidence > 0.95\n",
    "a lift score > 7\n",
    "We could compute the antecedent length as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3znKWtL8tTmG",
    "outputId": "e816655e-d4b2-41cb-eb5c-cd5247130eb3"
   },
   "outputs": [],
   "source": [
    "rules[\"antecedent_len\"] = rules[\"antecedents\"].apply(lambda x: len(x))\n",
    "rules[ (rules['antecedent_len'] >= 2) &\n",
    "       (rules['confidence'] > 0.95) &\n",
    "       (rules['lift'] > 7) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-Ibb1Og-UPX",
    "outputId": "cc994ffb-5f66-4c8c-a95e-0cc375123884"
   },
   "outputs": [],
   "source": [
    "plt.scatter(rules['support'], rules['confidence'], alpha=0.5)\n",
    "plt.xlabel('support')\n",
    "plt.ylabel('confidence')\n",
    "plt.title('Support vs Confidence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ar75W8E6_H1j",
    "outputId": "8bc087be-3840-4d0b-aa4a-46a2c46609d1"
   },
   "outputs": [],
   "source": [
    "plt.scatter(rules['support'], rules['lift'], alpha=0.5)\n",
    "plt.xlabel('support')\n",
    "plt.ylabel('lift')\n",
    "plt.title('Support vs lift')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBMlfnt-_nYk",
    "outputId": "c9c17730-ecbf-46f7-99e6-719b36e2e913"
   },
   "outputs": [],
   "source": [
    "plt.scatter(rules['lift'], rules['confidence'], alpha=0.5)\n",
    "plt.xlabel('lift')\n",
    "plt.ylabel('confidence')\n",
    "plt.title('lift vs confidence')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
